{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **View Pipeline Metadata**\n",
    "\n",
    "</center>\n",
    "\n",
    "# **Purpose**\n",
    "#### This notebook provides a view into the pipeline execution metadata, allowing you to:\n",
    "- Track pipeline runs and their status\n",
    "- Monitor row counts for each table\n",
    "- Identify failed runs and view error messages\n",
    "- Analyze execution durations and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependencies",
   "metadata": {},
   "source": [
    "# **Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet deltalake==0.18.2\n",
    "!pip install --quiet pandas\n",
    "!pip install --quiet matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-functions",
   "metadata": {},
   "source": [
    "# **Load Libraries and Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AquaQuiver_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable\n",
    "import pyarrow.compute as pc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "read-metadata",
   "metadata": {},
   "source": [
    "# **Read Metadata Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata table\n",
    "lakehouse_properties = notebookutils.lakehouse.get(\"Base\")\n",
    "abfss_path = lakehouse_properties[\"properties\"][\"abfsPath\"]\n",
    "delta_table_path = f\"{abfss_path}/Tables/pipeline_metadata\"\n",
    "\n",
    "aadToken = notebookutils.credentials.getToken('storage')\n",
    "storage_options={\"bearer_token\": aadToken, \"use_fabric_endpoint\": \"true\"}\n",
    "\n",
    "try:\n",
    "    dt = DeltaTable(delta_table_path, storage_options=storage_options)\n",
    "    metadata_arrow = dt.to_pyarrow_table()\n",
    "    metadata_df = metadata_arrow.to_pandas()\n",
    "    print(f\"Total metadata records: {len(metadata_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading metadata table: {str(e)}\")\n",
    "    metadata_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-runs",
   "metadata": {},
   "source": [
    "# **Recent Pipeline Runs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-recent",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty:\n",
    "    # Show last 10 runs\n",
    "    recent_runs = metadata_df.sort_values('run_timestamp', ascending=False).head(10)\n",
    "    display(recent_runs[['run_timestamp', 'notebook_name', 'table_name', 'layer', 'row_count', 'status', 'duration_seconds']])\n",
    "else:\n",
    "    print(\"No metadata records found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-summary",
   "metadata": {},
   "source": [
    "# **Pipeline Run Summary by Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty:\n",
    "    # Group by run_id to show session summary\n",
    "    session_summary = metadata_df.groupby('run_id').agg({\n",
    "        'run_timestamp': 'min',\n",
    "        'notebook_name': 'count',\n",
    "        'row_count': 'sum',\n",
    "        'status': lambda x: 'FAILED' if 'FAILED' in x.values else 'SUCCESS',\n",
    "        'duration_seconds': 'max'\n",
    "    }).rename(columns={\n",
    "        'run_timestamp': 'session_start',\n",
    "        'notebook_name': 'tables_processed',\n",
    "        'row_count': 'total_rows',\n",
    "        'status': 'session_status',\n",
    "        'duration_seconds': 'total_duration'\n",
    "    })\n",
    "    \n",
    "    session_summary = session_summary.sort_values('session_start', ascending=False).head(10)\n",
    "    print(\"\\nRecent Pipeline Sessions:\")\n",
    "    display(session_summary)\n",
    "else:\n",
    "    print(\"No session data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failed-runs",
   "metadata": {},
   "source": [
    "# **Failed Runs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-failures",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty:\n",
    "    failed_runs = metadata_df[metadata_df['status'] == 'FAILED']\n",
    "    \n",
    "    if not failed_runs.empty:\n",
    "        print(f\"Found {len(failed_runs)} failed runs:\")\n",
    "        display(failed_runs[['run_timestamp', 'notebook_name', 'table_name', 'error_message']].sort_values('run_timestamp', ascending=False))\n",
    "    else:\n",
    "        print(\"No failed runs found!\")\n",
    "else:\n",
    "    print(\"No metadata records found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-metrics",
   "metadata": {},
   "source": [
    "# **Performance Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty:\n",
    "    # Calculate average processing time by table\n",
    "    avg_duration = metadata_df[metadata_df['status'] == 'SUCCESS'].groupby('table_name')['duration_seconds'].agg(['mean', 'min', 'max', 'count'])\n",
    "    avg_duration.columns = ['avg_seconds', 'min_seconds', 'max_seconds', 'run_count']\n",
    "    avg_duration = avg_duration.sort_values('avg_seconds', ascending=False)\n",
    "    \n",
    "    print(\"\\nProcessing Time by Table:\")\n",
    "    display(avg_duration)\n",
    "    \n",
    "    # Plot if we have data\n",
    "    if len(avg_duration) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Bar chart of average processing time\n",
    "        avg_duration['avg_seconds'].plot(kind='barh', ax=ax1)\n",
    "        ax1.set_title('Average Processing Time by Table')\n",
    "        ax1.set_xlabel('Seconds')\n",
    "        \n",
    "        # Row counts by table\n",
    "        avg_rows = metadata_df[metadata_df['status'] == 'SUCCESS'].groupby('table_name')['row_count'].mean().sort_values(ascending=False)\n",
    "        avg_rows.plot(kind='barh', ax=ax2)\n",
    "        ax2.set_title('Average Row Count by Table')\n",
    "        ax2.set_xlabel('Rows')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No metadata records found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-series",
   "metadata": {},
   "source": [
    "# **Pipeline Execution Timeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timeline-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty and len(metadata_df) > 1:\n",
    "    # Create timeline plot\n",
    "    metadata_df['run_timestamp'] = pd.to_datetime(metadata_df['run_timestamp'])\n",
    "    \n",
    "    # Group by date and count runs\n",
    "    daily_runs = metadata_df.groupby(metadata_df['run_timestamp'].dt.date).agg({\n",
    "        'notebook_name': 'count',\n",
    "        'row_count': 'sum',\n",
    "        'status': lambda x: (x == 'FAILED').sum()\n",
    "    }).rename(columns={'notebook_name': 'total_runs', 'row_count': 'total_rows', 'status': 'failed_runs'})\n",
    "    \n",
    "    if len(daily_runs) > 0:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "        \n",
    "        # Plot runs per day\n",
    "        daily_runs[['total_runs', 'failed_runs']].plot(ax=axes[0], kind='bar', stacked=False)\n",
    "        axes[0].set_title('Pipeline Executions Per Day')\n",
    "        axes[0].set_xlabel('Date')\n",
    "        axes[0].set_ylabel('Number of Runs')\n",
    "        axes[0].legend(['Total Runs', 'Failed Runs'])\n",
    "        \n",
    "        # Plot rows processed per day\n",
    "        daily_runs['total_rows'].plot(ax=axes[1], kind='line', marker='o')\n",
    "        axes[1].set_title('Total Rows Processed Per Day')\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].set_ylabel('Row Count')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for timeline visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
